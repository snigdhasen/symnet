Scaling Technique: Sum of each feature=1
Activation in final Layer: ReLU
Batch Normalisation done
ResNet architecture with ReLU as activation functions

y_train is of the order 10^-7
training loss is of the order 10^-7
validation loss is of the order 10^-4
Network suffers from vanishing gradient problem
